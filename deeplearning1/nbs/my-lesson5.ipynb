{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import utils\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, BatchNormalization, Embedding, merge, Flatten, Dropout, Convolution1D, MaxPooling1D\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import mean_absolute_percentage_error\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from keras.metrics import binary_accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ddir=\"/home/ajith/dl/deeplearning1/nbs/data/imdb/\"\n",
    "ddir=\"/home/ubuntu/nbs/courses/deeplearning1/nbs/data/imdb/\"\n",
    "rdir=ddir + \"results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A local file was found, but it seems to be incomplete or outdated.\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.pkl\n",
      "2228224/2343108 [===========================>..] - ETA: 0sA local file was found, but it seems to be incomplete or outdated.\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_full.pkl\n",
      "65495040/65552540 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "word2id = imdb.get_word_index(path=ddir + \"imdb_full.pkl\")\n",
    "path = get_file(ddir +'imdb_full.pkl',\n",
    "                origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\n",
    "                md5_hash='d091312047c43cf9e4e38fef92437263')\n",
    "f = open(path, 'rb')\n",
    "(x_train, y_train), (x_test, y_test) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "id2word = {v:k for (k, v) in word2id.iteritems()} # dictionary from index to corresponding word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def to_text(code):\n",
    "    text=''.join([id2word[i] + \" \" for i in code])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Shorten reviews by removing most common words(like 'the', 'and') etc that do not necessarily add to sentiment\n",
    "# Remove least frequently used words too\n",
    "\n",
    "FROM=5\n",
    "TO=5000\n",
    "VOCAB_SIZE=TO\n",
    "def reduce_vocab(code):\n",
    "    #new_code = [np.array([val if (val >= FROM and val < TO) else (TO-1) for val in s]) for s in code]\n",
    "    new_code = [np.array([val for val in s if (val >= FROM and val < TO)]) for s in code]\n",
    "    return new_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_trainc = reduce_vocab(x_train)\n",
    "x_testc = reduce_vocab(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 500)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truncate train and test data to a 500-word wide matrix by truncating/padding as appropriate.\n",
    "SEQ_LEN=500\n",
    "x_trainc = pad_sequences(x_trainc, maxlen=SEQ_LEN, value=0)\n",
    "x_testc = pad_sequences(x_testc, maxlen=SEQ_LEN, value=0)\n",
    "x_trainc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Simple linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_5 (Embedding)          (None, 500, 32)       160000      embedding_input_5[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 16000)         0           embedding_5[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 16000)         0           flatten_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_9 (BatchNormal(None, 16000)         32000       dropout_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 100)           1600100     batchnormalization_9[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 100)           0           dense_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_10 (BatchNorma(None, 100)           200         dropout_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 1)             101         batchnormalization_10[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 1792401\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nfactors=32\n",
    "model = Sequential([\n",
    "        Embedding(VOCAB_SIZE, nfactors, input_length=SEQ_LEN, W_regularizer=l2(1e-4)),\n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "        BatchNormalization(),\n",
    "        Dense(100, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        BatchNormalization(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 7s - loss: 0.3356 - acc: 0.8761 - val_loss: 0.2932 - val_acc: 0.8736\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 6s - loss: 0.2473 - acc: 0.9131 - val_loss: 0.3040 - val_acc: 0.8726\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 6s - loss: 0.2054 - acc: 0.9331 - val_loss: 0.3063 - val_acc: 0.8771\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 6s - loss: 0.1877 - acc: 0.9419 - val_loss: 0.3273 - val_acc: 0.8715\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 6s - loss: 0.1733 - acc: 0.9503 - val_loss: 0.3596 - val_acc: 0.8641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0a1affccd0>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_trainc, y_train, batch_size=64, nb_epoch=5, validation_data=(x_testc, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_29 (Embedding)         (None, 500, 32)       160000      embedding_input_29[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)             (None, 500, 32)       0           embedding_29[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_29 (Convolution1D) (None, 500, 64)       10304       dropout_61[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)             (None, 32000)         0           convolution1d_29[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_44 (BatchNorma(None, 32000)         64000       flatten_29[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)             (None, 32000)         0           batchnormalization_44[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_56 (Dense)                 (None, 100)           3200100     dropout_62[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_45 (BatchNorma(None, 100)           200         dense_56[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)             (None, 100)           0           batchnormalization_45[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_57 (Dense)                 (None, 1)             101         dropout_63[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 3434705\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nfactors=32\n",
    "\n",
    "cm = Sequential([\n",
    "        Embedding(VOCAB_SIZE, nfactors, input_length=SEQ_LEN, W_regularizer=l2(1e-4), dropout=0.2),\n",
    "        Dropout(0.2),\n",
    "        Convolution1D(64, 5, border_mode='same', activation='relu'),\n",
    "        Flatten(),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(100, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "cm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 14s - loss: 0.2011 - acc: 0.9344 - val_loss: 0.3175 - val_acc: 0.8752\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 14s - loss: 0.1840 - acc: 0.9422 - val_loss: 0.3227 - val_acc: 0.8750\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 14s - loss: 0.1803 - acc: 0.9456 - val_loss: 0.3101 - val_acc: 0.8776\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 14s - loss: 0.1666 - acc: 0.9533 - val_loss: 0.3225 - val_acc: 0.8768\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 14s - loss: 0.1608 - acc: 0.9551 - val_loss: 0.3408 - val_acc: 0.8714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f09ccf8cd50>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.fit(x_trainc, y_train, batch_size=64, nb_epoch=5, validation_data=(x_testc, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Pre-trained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vectors(loc):\n",
    "    return (utils.load_array(loc+'.dat'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb')),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GPATH=\"/home/ubuntu/nbs/courses/deeplearning1/nbs/data/glove/6B.100d\"\n",
    "vecs, words, wordidx = load_vectors(GPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create embedding matching word ids in IMDB data\n",
    "def create_embedding():\n",
    "    no_word_cnt=0\n",
    "    nfactors = vecs.shape[1]\n",
    "    emb = np.zeros((VOCAB_SIZE, nfactors))\n",
    "    for imdb_id in range(1, VOCAB_SIZE):\n",
    "        imdb_word=id2word[imdb_id]\n",
    "        if wordidx.has_key(imdb_word):\n",
    "            emb[imdb_id] = vecs[wordidx.get(imdb_word)]\n",
    "        else:\n",
    "            emb[imdb_id] = np.random.normal(scale=0.6, size=(nfactors,))\n",
    "            no_word_cnt+=1\n",
    "    print(\"Created embedding with no_word_cnt=%d\" %no_word_cnt)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embedding with no_word_cnt=79\n"
     ]
    }
   ],
   "source": [
    "glovemb = create_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_36 (Embedding)         (None, 500, 100)      500000      embedding_input_36[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_59 (BatchNorma(None, 500, 100)      200         embedding_36[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_82 (Dropout)             (None, 500, 100)      0           batchnormalization_59[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_36 (Convolution1D) (None, 500, 32)       16032       dropout_82[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "flatten_36 (Flatten)             (None, 16000)         0           convolution1d_36[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_60 (BatchNorma(None, 16000)         32000       flatten_36[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_83 (Dropout)             (None, 16000)         0           batchnormalization_60[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_70 (Dense)                 (None, 100)           1600100     dropout_83[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_61 (BatchNorma(None, 100)           200         dense_70[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_84 (Dropout)             (None, 100)           0           batchnormalization_61[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_71 (Dense)                 (None, 1)             101         dropout_84[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 2148633\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nfactors = vecs.shape[1]\n",
    "cm = Sequential([\n",
    "        Embedding(VOCAB_SIZE, nfactors, input_length=SEQ_LEN, W_regularizer=l2(1e-4), weights=[glovemb]),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Convolution1D(32, 5, border_mode='same', activation='relu'),\n",
    "        Flatten(),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(100, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "cm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 22s - loss: 7.7146 - acc: 0.8187 - val_loss: 0.3174 - val_acc: 0.8647\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 22s - loss: 3.8661 - acc: 0.8748 - val_loss: 0.2955 - val_acc: 0.8734\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 22s - loss: 2.0993 - acc: 0.8943 - val_loss: 0.2768 - val_acc: 0.8854\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 22s - loss: 1.2731 - acc: 0.9084 - val_loss: 0.2848 - val_acc: 0.8812\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 22s - loss: 0.8619 - acc: 0.9163 - val_loss: 0.2897 - val_acc: 0.8786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f09a043a410>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.fit(x_trainc, y_train, batch_size=64, nb_epoch=5, validation_data=(x_testc, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
